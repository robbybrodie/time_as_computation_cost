{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/robbybrodie/time_as_computation_cost/blob/main/notebooks/bandgaps_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Bandgaps Experiment: DoF Law Fitting\n",
    "\n",
    "This experiment fits microphysical degrees of freedom (DoF) laws and beta parameters from synthetic data, comparing our computational-capacity model against baseline models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theory"
   },
   "source": [
    "## Theory\n",
    "\n",
    "- **DoF Law**: `DoF(N) = exp[-a(1-N)]` where N is computational capacity\n",
    "- **Beta Parameter**: `β(ΔF) = β₀ + β₁*ΔF` relating to micro-physical processes  \n",
    "- **Model Selection**: AIC/BIC comparison against polynomial and power-law baselines\n",
    "\n",
    "**Physical Interpretation**: The degrees of freedom available for physical processes depend exponentially on the computational capacity N. As N approaches 1 (full capacity), more degrees of freedom become accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup: Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bootstrap"
   },
   "outputs": [],
   "source": [
    "# Clone and setup (idempotent)\n",
    "import os, sys, subprocess, shutil, pathlib\n",
    "REPO_URL = \"https://github.com/robbybrodie/time_as_computation_cost.git\"\n",
    "REPO_NAME = \"time_as_computation_cost\"\n",
    "\n",
    "if not pathlib.Path(REPO_NAME).exists():\n",
    "    !git clone $REPO_URL\n",
    "\n",
    "%cd $REPO_NAME\n",
    "\n",
    "# Install package\n",
    "if (pathlib.Path(\"pyproject.toml\")).exists():\n",
    "    !pip install -e .\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "import numpy as np, random\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "experiment"
   },
   "source": [
    "## Run Bandgaps Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_experiment"
   },
   "outputs": [],
   "source": [
    "from experiments.run_bandgaps import main\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "display"
   },
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_results"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Display generated plots\n",
    "output_dir = Path(\"experiments/out/bandgaps\")\n",
    "\n",
    "print(\"DoF and Beta Parameter Fits:\")\n",
    "display(Image(str(output_dir / \"bandgaps_fits.png\")))\n",
    "\n",
    "print(\"\\nModel Comparison with Baselines:\")\n",
    "display(Image(str(output_dir / \"baseline_comparison.png\")))\n",
    "\n",
    "# Display numerical results\n",
    "print(\"\\nNumerical Results:\")\n",
    "with open(output_dir / \"results.txt\", 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_interactive"
   },
   "source": [
    "## Interactive Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive_setup"
   },
   "outputs": [],
   "source": [
    "# Setup for interactive exploration\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Ensure we can import the modules\n",
    "repo_root = Path().resolve()\n",
    "sys.path.insert(0, str(repo_root / \"src\"))\n",
    "\n",
    "from tacc.micro.bandgaps import fit_dof_law, fit_beta\n",
    "from tacc.baselines import compare_models\n",
    "from experiments.run_bandgaps import main\n",
    "\n",
    "print(\"Interactive modules loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive_dof"
   },
   "source": [
    "## Interactive DoF Law Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore_dof"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data with different parameters\n",
    "def explore_dof_law(true_a=2.0, noise_level=0.05, n_points=50):\n",
    "    \"\"\"Interactive exploration of DoF law fitting\"\"\"\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    N_values = np.linspace(0.5, 1.5, n_points)\n",
    "    DoF_true = np.exp(-true_a * (1 - N_values))\n",
    "    DoF_noisy = DoF_true + np.random.normal(0, noise_level, len(DoF_true))\n",
    "    \n",
    "    # Fit our exponential model\n",
    "    fitted_a, fit_info = fit_dof_law(N_values, DoF_noisy)\n",
    "    \n",
    "    # Fit baseline models for comparison\n",
    "    from scipy.optimize import curve_fit\n",
    "    \n",
    "    # Polynomial model\n",
    "    def poly_model(N, a, b, c):\n",
    "        return a + b*N + c*N**2\n",
    "    poly_params, _ = curve_fit(poly_model, N_values, DoF_noisy, maxfev=5000)\n",
    "    \n",
    "    # Power law model  \n",
    "    def power_model(N, a, b):\n",
    "        return a * np.power(N, b)\n",
    "    try:\n",
    "        power_params, _ = curve_fit(power_model, N_values, DoF_noisy, maxfev=5000)\n",
    "    except:\n",
    "        power_params = [1.0, 1.0]  # fallback\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Main fit comparison\n",
    "    N_fine = np.linspace(0.5, 1.5, 200)\n",
    "    DoF_fit = np.exp(-fitted_a * (1 - N_fine))\n",
    "    poly_fit = poly_model(N_fine, *poly_params)\n",
    "    power_fit = power_model(N_fine, *power_params)\n",
    "    \n",
    "    ax1.scatter(N_values, DoF_noisy, alpha=0.6, color='gray', label='Synthetic data')\n",
    "    ax1.plot(N_fine, np.exp(-true_a * (1 - N_fine)), 'k--', linewidth=2, label=f'True (a={true_a})')\n",
    "    ax1.plot(N_fine, DoF_fit, 'r-', linewidth=2, label=f'Exponential fit (a={fitted_a:.3f})')\n",
    "    ax1.plot(N_fine, poly_fit, 'b-', linewidth=2, label='Polynomial')\n",
    "    ax1.plot(N_fine, power_fit, 'g-', linewidth=2, label='Power law')\n",
    "    ax1.set_xlabel('N (Computational Capacity)')\n",
    "    ax1.set_ylabel('DoF')\n",
    "    ax1.set_title('DoF Law Fitting Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals for our model\n",
    "    DoF_pred = np.exp(-fitted_a * (1 - N_values))\n",
    "    residuals = DoF_noisy - DoF_pred\n",
    "    ax2.scatter(N_values, residuals, alpha=0.7, color='red')\n",
    "    ax2.axhline(y=0, color='k', linestyle='--', alpha=0.7)\n",
    "    ax2.set_xlabel('N')\n",
    "    ax2.set_ylabel('Residuals')\n",
    "    ax2.set_title('Exponential Model Residuals')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Parameter sensitivity\n",
    "    a_range = np.linspace(0.5, 4.0, 100)\n",
    "    DoF_sensitivity = [np.exp(-a * (1 - 1.0)) for a in a_range]  # At N=1\n",
    "    ax3.plot(a_range, DoF_sensitivity, 'purple', linewidth=2)\n",
    "    ax3.axvline(x=true_a, color='k', linestyle='--', label=f'True a={true_a}')\n",
    "    ax3.axvline(x=fitted_a, color='r', linestyle=':', label=f'Fitted a={fitted_a:.3f}')\n",
    "    ax3.set_xlabel('Parameter a')\n",
    "    ax3.set_ylabel('DoF(N=1)')\n",
    "    ax3.set_title('Parameter Sensitivity')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error vs noise level\n",
    "    noise_levels = np.logspace(-3, -1, 20)\n",
    "    errors = []\n",
    "    for noise in noise_levels:\n",
    "        DoF_test = DoF_true + np.random.normal(0, noise, len(DoF_true))\n",
    "        fitted_a_test, _ = fit_dof_law(N_values, DoF_test)\n",
    "        errors.append(abs(fitted_a_test - true_a))\n",
    "    \n",
    "    ax4.loglog(noise_levels, errors, 'o-', color='orange', linewidth=2)\n",
    "    ax4.axhline(y=0.1, color='r', linestyle='--', alpha=0.7, label='10% error')\n",
    "    ax4.set_xlabel('Noise Level')\n",
    "    ax4.set_ylabel('Parameter Error |â - a|')\n",
    "    ax4.set_title('Noise Sensitivity')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comparison metrics\n",
    "    print(f\"Parameter Recovery:\")\n",
    "    print(f\"  True a = {true_a:.3f}\")\n",
    "    print(f\"  Fitted a = {fitted_a:.3f}\")\n",
    "    print(f\"  Error = {abs(fitted_a - true_a):.3f}\")\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"  RMSE = {np.sqrt(np.mean(residuals**2)):.6f}\")\n",
    "    print(f\"  Max residual = {np.max(np.abs(residuals)):.6f}\")\n",
    "\n",
    "# Explore with different parameters\n",
    "print(\"Exploring DoF law with default parameters:\")\n",
    "np.random.seed(42)\n",
    "explore_dof_law()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Exploring with higher noise:\")\n",
    "np.random.seed(42)\n",
    "explore_dof_law(true_a=1.5, noise_level=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "params"
   },
   "source": [
    "## Basic Parameter Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_params"
   },
   "outputs": [],
   "source": [
    "# Custom parameters for exploration\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tacc.micro.bandgaps import fit_dof_law, fit_beta\n",
    "\n",
    "# Try different noise levels\n",
    "noise_levels = [0.01, 0.05, 0.1, 0.2]\n",
    "fitted_params = []\n",
    "\n",
    "for noise in noise_levels:\n",
    "    np.random.seed(42)\n",
    "    N_values = np.linspace(0.5, 1.5, 50)\n",
    "    true_a = 2.0\n",
    "    DoF_values = np.exp(-true_a * (1 - N_values))\n",
    "    DoF_values += np.random.normal(0, noise, len(DoF_values))\n",
    "    \n",
    "    # Fit with custom data\n",
    "    from scipy.optimize import curve_fit\n",
    "    def dof_model(N, a):\n",
    "        return np.exp(-a * (1 - N))\n",
    "    \n",
    "    popt, _ = curve_fit(dof_model, N_values, DoF_values)\n",
    "    fitted_params.append(popt[0])\n",
    "    print(f\"Noise level {noise:.2f}: fitted a = {popt[0]:.3f} (error: {abs(popt[0] - true_a):.3f})\")\n",
    "\n",
    "# Plot noise sensitivity\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(noise_levels, fitted_params, 'bo-', linewidth=2, markersize=8)\n",
    "plt.axhline(y=2.0, color='r', linestyle='--', label='True value (a=2.0)')\n",
    "plt.xlabel('Noise Level')\n",
    "plt.ylabel('Fitted Parameter a')\n",
    "plt.title('Noise Sensitivity Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beta_analysis"
   },
   "source": [
    "## Beta Parameter Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beta_exploration"
   },
   "outputs": [],
   "source": [
    "def explore_beta_parameter(true_beta0=1.5, true_beta1=0.3, noise_level=0.02):\n",
    "    \"\"\"Interactive exploration of beta parameter fitting\"\"\"\n",
    "    \n",
    "    # Generate synthetic beta data\n",
    "    Delta_F_values = np.linspace(-0.5, 0.5, 40)\n",
    "    beta_true = true_beta0 + true_beta1 * Delta_F_values\n",
    "    beta_noisy = beta_true + np.random.normal(0, noise_level, len(beta_true))\n",
    "    \n",
    "    # Fit beta parameters\n",
    "    fitted_beta0, fitted_beta1, fit_info = fit_beta(Delta_F_values, beta_noisy)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Main fit\n",
    "    Delta_F_fine = np.linspace(-0.5, 0.5, 200)\n",
    "    beta_fit = fitted_beta0 + fitted_beta1 * Delta_F_fine\n",
    "    \n",
    "    ax1.scatter(Delta_F_values, beta_noisy, alpha=0.6, color='gray', label='Synthetic data')\n",
    "    ax1.plot(Delta_F_fine, true_beta0 + true_beta1 * Delta_F_fine, 'k--', linewidth=2, \n",
    "             label=f'True (β₀={true_beta0}, β₁={true_beta1})')\n",
    "    ax1.plot(Delta_F_fine, beta_fit, 'r-', linewidth=2,\n",
    "             label=f'Fit (β₀={fitted_beta0:.3f}, β₁={fitted_beta1:.3f})')\n",
    "    ax1.set_xlabel('ΔF')\n",
    "    ax1.set_ylabel('β')\n",
    "    ax1.set_title('Beta Parameter Fitting')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals\n",
    "    beta_pred = fitted_beta0 + fitted_beta1 * Delta_F_values\n",
    "    residuals = beta_noisy - beta_pred\n",
    "    ax2.scatter(Delta_F_values, residuals, alpha=0.7, color='red')\n",
    "    ax2.axhline(y=0, color='k', linestyle='--', alpha=0.7)\n",
    "    ax2.set_xlabel('ΔF')\n",
    "    ax2.set_ylabel('Residuals')\n",
    "    ax2.set_title('Beta Model Residuals')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Parameter correlation\n",
    "    ax3.scatter([true_beta0], [true_beta1], s=100, color='black', marker='x', \n",
    "               label='True parameters', zorder=10)\n",
    "    ax3.scatter([fitted_beta0], [fitted_beta1], s=100, color='red', marker='o',\n",
    "               label='Fitted parameters', zorder=10)\n",
    "    ax3.set_xlabel('β₀')\n",
    "    ax3.set_ylabel('β₁')\n",
    "    ax3.set_title('Parameter Space')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confidence intervals (approximate)\n",
    "    # Bootstrap sampling\n",
    "    n_bootstrap = 100\n",
    "    beta0_boots, beta1_boots = [], []\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = np.random.choice(len(Delta_F_values), len(Delta_F_values), replace=True)\n",
    "        boot_DeltaF = Delta_F_values[indices]\n",
    "        boot_beta = beta_noisy[indices]\n",
    "        boot_beta0, boot_beta1, _ = fit_beta(boot_DeltaF, boot_beta)\n",
    "        beta0_boots.append(boot_beta0)\n",
    "        beta1_boots.append(boot_beta1)\n",
    "    \n",
    "    ax4.scatter(beta0_boots, beta1_boots, alpha=0.3, color='blue', s=10)\n",
    "    ax4.scatter([fitted_beta0], [fitted_beta1], s=100, color='red', marker='o', zorder=10)\n",
    "    ax4.set_xlabel('β₀')  \n",
    "    ax4.set_ylabel('β₁')\n",
    "    ax4.set_title('Bootstrap Parameter Distribution')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Beta Parameter Recovery:\")\n",
    "    print(f\"  True β₀ = {true_beta0:.3f}, β₁ = {true_beta1:.3f}\")\n",
    "    print(f\"  Fitted β₀ = {fitted_beta0:.3f}, β₁ = {fitted_beta1:.3f}\")\n",
    "    print(f\"  Errors: Δβ₀ = {abs(fitted_beta0-true_beta0):.3f}, Δβ₁ = {abs(fitted_beta1-true_beta1):.3f}\")\n",
    "\n",
    "print(\"Exploring beta parameter fitting:\")\n",
    "np.random.seed(42)\n",
    "explore_beta_parameter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_comparison"
   },
   "source": [
    "## Comprehensive Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "comprehensive_comparison"
   },
   "outputs": [],
   "source": [
    "def comprehensive_model_comparison():\n",
    "    \"\"\"Compare exponential model against multiple baselines\"\"\"\n",
    "    \n",
    "    # Generate test data\n",
    "    np.random.seed(42)\n",
    "    N_values = np.linspace(0.5, 1.5, 50)\n",
    "    true_a = 2.0\n",
    "    DoF_true = np.exp(-true_a * (1 - N_values))\n",
    "    DoF_noisy = DoF_true + np.random.normal(0, 0.05, len(DoF_true))\n",
    "    \n",
    "    # Define models\n",
    "    def exponential_model(N, a):\n",
    "        return np.exp(-a * (1 - N))\n",
    "    \n",
    "    def polynomial2_model(N, a, b, c):\n",
    "        return a + b*N + c*N**2\n",
    "        \n",
    "    def polynomial3_model(N, a, b, c, d):\n",
    "        return a + b*N + c*N**2 + d*N**3\n",
    "    \n",
    "    def power_model(N, a, b):\n",
    "        return a * np.power(np.maximum(N, 1e-6), b)  # Avoid zero\n",
    "    \n",
    "    def rational_model(N, a, b, c):\n",
    "        return (a + b*N) / (1 + c*N)\n",
    "    \n",
    "    models = {\n",
    "        'Exponential': (exponential_model, [2.0]),\n",
    "        'Polynomial-2': (polynomial2_model, [1.0, 0.0, 0.0]),\n",
    "        'Polynomial-3': (polynomial3_model, [1.0, 0.0, 0.0, 0.0]), \n",
    "        'Power Law': (power_model, [1.0, 1.0]),\n",
    "        'Rational': (rational_model, [1.0, 0.0, 0.0])\n",
    "    }\n",
    "    \n",
    "    # Fit all models and calculate metrics\n",
    "    results = {}\n",
    "    from scipy.optimize import curve_fit\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    N_fine = np.linspace(0.5, 1.5, 200)\n",
    "    \n",
    "    aic_values, bic_values, rmse_values = [], [], []\n",
    "    model_names = []\n",
    "    \n",
    "    for i, (name, (model_func, initial_params)) in enumerate(models.items()):\n",
    "        try:\n",
    "            # Fit model\n",
    "            params, pcov = curve_fit(model_func, N_values, DoF_noisy, \n",
    "                                   p0=initial_params, maxfev=10000)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            DoF_pred = model_func(N_values, *params)\n",
    "            n_params = len(params)\n",
    "            n_data = len(N_values)\n",
    "            \n",
    "            # Log-likelihood (assuming Gaussian noise)\n",
    "            mse = np.mean((DoF_noisy - DoF_pred)**2)\n",
    "            rmse = np.sqrt(mse)\n",
    "            log_likelihood = -0.5 * n_data * np.log(2 * np.pi * mse) - 0.5 * np.sum((DoF_noisy - DoF_pred)**2) / mse\n",
    "            \n",
    "            # Information criteria\n",
    "            aic = 2 * n_params - 2 * log_likelihood\n",
    "            bic = n_params * np.log(n_data) - 2 * log_likelihood\n",
    "            \n",
    "            results[name] = {\n",
    "                'params': params,\n",
    "                'AIC': aic,\n",
    "                'BIC': bic,\n",
    "                'RMSE': rmse,\n",
    "                'predictions': model_func(N_fine, *params)\n",
    "            }\n",
    "            \n",
    "            aic_values.append(aic)\n",
    "            bic_values.append(bic)  \n",
    "            rmse_values.append(rmse)\n",
    "            model_names.append(name)\n",
    "            \n",
    "            # Plot fit\n",
    "            ax1.plot(N_fine, model_func(N_fine, *params), color=colors[i], \n",
    "                    linewidth=2, label=f'{name} (AIC={aic:.1f})')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fit {name}: {e}\")\n",
    "    \n",
    "    # Plot data and true function\n",
    "    ax1.scatter(N_values, DoF_noisy, alpha=0.6, color='gray', label='Data', zorder=10)\n",
    "    ax1.plot(N_fine, exponential_model(N_fine, true_a), 'k--', linewidth=3, \n",
    "            label='True function', zorder=5)\n",
    "    ax1.set_xlabel('N (Computational Capacity)')\n",
    "    ax1.set_ylabel('DoF') \n",
    "    ax1.set_title('Model Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # AIC comparison\n",
    "    ax2.bar(range(len(model_names)), aic_values, color=colors[:len(model_names)])\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('AIC (lower is better)')\n",
    "    ax2.set_title('Akaike Information Criterion')\n",
    "    ax2.set_xticks(range(len(model_names)))\n",
    "    ax2.set_xticklabels(model_names, rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # BIC comparison  \n",
    "    ax3.bar(range(len(model_names)), bic_values, color=colors[:len(model_names)])\n",
    "    ax3.set_xlabel('Model')\n",
    "    ax3.set_ylabel('BIC (lower is better)')\n",
    "    ax3.set_title('Bayesian Information Criterion')\n",
    "    ax3.set_xticks(range(len(model_names)))\n",
    "    ax3.set_xticklabels(model_names, rotation=45)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # RMSE comparison\n",
    "    ax4.bar(range(len(model_names)), rmse_values, color=colors[:len(model_names)])\n",
    "    ax4.set_xlabel('Model')\n",
    "    ax4.set_ylabel('RMSE (lower is better)')\n",
    "    ax4.set_title('Root Mean Square Error')\n",
    "    ax4.set_xticks(range(len(model_names)))\n",
    "    ax4.set_xticklabels(model_names, rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print ranking\n",
    "    print(\"Model Ranking by AIC (lower is better):\")\n",
    "    aic_ranking = sorted(zip(model_names, aic_values), key=lambda x: x[1])\n",
    "    for i, (name, aic) in enumerate(aic_ranking):\n",
    "        print(f\"  {i+1}. {name}: AIC = {aic:.2f}\")\n",
    "    \n",
    "    print(\"\\nModel Ranking by BIC (lower is better):\")  \n",
    "    bic_ranking = sorted(zip(model_names, bic_values), key=lambda x: x[1])\n",
    "    for i, (name, bic) in enumerate(bic_ranking):\n",
    "        print(f\"  {i+1}. {name}: BIC = {bic:.2f}\")\n",
    "\n",
    "print(\"Running comprehensive model comparison:\")\n",
    "comprehensive_model_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "insights"
   },
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Parameter Recovery**: The exponential model can accurately recover the true parameter `a` from noisy synthetic data\n",
    "\n",
    "2. **Model Selection**: AIC/BIC metrics help distinguish between competing functional forms\n",
    "\n",
    "3. **Noise Sensitivity**: Parameter accuracy degrades predictably with increasing noise levels\n",
    "\n",
    "4. **Beta Parameters**: Linear relationships in micro-physical processes can be reliably detected\n",
    "\n",
    "5. **Baseline Comparison**: Our exponential form often outperforms polynomial models for this type of data\n",
    "\n",
    "## Physical Interpretation\n",
    "\n",
    "- **DoF(N)**: Represents available degrees of freedom as function of computational capacity\n",
    "- **Beta Parameters**: Characterize micro-physical response to external perturbations  \n",
    "- **Model Selection**: Determines which mathematical form best captures underlying physics\n",
    "\n",
    "**Note**: This is synthetic data analysis for a conceptual framework - not established physics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "**Common Issues:**\n",
    "- If plots don't display, ensure matplotlib is properly installed\n",
    "- If imports fail, check that the repository was cloned correctly\n",
    "- For fitting errors, verify that the synthetic data generation is working\n",
    "\n",
    "**Expected Output:**\n",
    "- Multiple comprehensive interactive analyses with 4-panel plots\n",
    "- Parameter recovery demonstrations with noise sensitivity analysis\n",
    "- Model comparison rankings using AIC/BIC criteria\n",
    "- Bootstrap confidence interval analysis for parameter uncertainties"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
